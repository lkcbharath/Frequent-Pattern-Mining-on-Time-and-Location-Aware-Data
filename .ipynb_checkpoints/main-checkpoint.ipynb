{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "location_dict = defaultdict(None)\n",
    "pollutant_dict = defaultdict(None)\n",
    "time_dict = defaultdict(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_star_cmp(n):\n",
    "#     return (n+1)*unique_id\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summary: 3-D matrix (locations, pollutants, quartiles)\n",
    "## samples: 2-D matrix (locations, pollutants)\n",
    "## pollutant_names: 1-D array\n",
    "def preprocessing(samples, summary, pollutant_names):\n",
    "    database_codes = []\n",
    "\n",
    "    for i in len(dataset):\n",
    "        segment = dataset[i]\n",
    "        for j in len(segment):\n",
    "            attribute_value = segment[j]\n",
    "\n",
    "            minimum = summary[i][j][0]\n",
    "            first_quantile = summary[i][j][1]\n",
    "            third_quantile = summary[i][j][3]\n",
    "\n",
    "            if (attribute_value >= minimum) and (attribute_value < first_quantile):\n",
    "                database_codes[i][j] = pollutant_names[j] + \"1\"\n",
    "            elif (attribute_value >= first_quantile) and (attribute_value < third_quantile):\n",
    "                database_codes[i][j] = pollutant_names[j] + \"2\"\n",
    "            else:\n",
    "                database_codes[i][j] = pollutant_names[j] + \"3\"\n",
    "\n",
    "    return database_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## locations: 1-D array containing names of locations\n",
    "## pollutants: 1-D array containing names of pollutants\n",
    "## times: 1-D array containing names of time measures (January, February etc.)\n",
    "def dict_generator(locations, pollutants, times):\n",
    "    for i in range(len(locations)):\n",
    "        location_dict[locations[i]] = i\n",
    "    for j in range(len(times)):\n",
    "        time_dict[times[j]] = j\n",
    "    for k in range(len(pollutants)):\n",
    "        pollutant_dict[pollutants[k]] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "## locations: 1-D array containing names of locations\n",
    "## times: 1-D array containing names of time measures (January, February etc.)\n",
    "def calendar_map_schema_generator(transactions):\n",
    "    \n",
    "    # correct it to add only stars for possible locations and times. e.g. s2 doesnt need stars\n",
    "    locations = transactions['location'].unique()\n",
    "    times = transactions['time'].unique()\n",
    "    \n",
    "    location_star_keys = set()\n",
    "    for location in locations:\n",
    "        rows = transactions.loc[transactions['location'] == location]\n",
    "        unique_times = rows['time'].unique()\n",
    "        if len(unique_times) > 1:\n",
    "            location_star_keys.add(location)\n",
    "    \n",
    "    time_star_keys = set()\n",
    "    for time in times:\n",
    "        rows = transactions.loc[transactions['time'] == time]\n",
    "        unique_locations = rows['location'].unique()\n",
    "        if len(unique_locations) > 1:\n",
    "            time_star_keys.add(time)\n",
    "    \n",
    "    location_stars = np.array(['*' for i in range(len(location_star_keys))])\n",
    "    time_stars = np.array(['*' for i in range(len(time_star_keys))])\n",
    "    \n",
    "    calendar_map_schema = pd.DataFrame(columns=['location', 'time', 'sid', 'hash_id'])\n",
    "    \n",
    "    sid = 0\n",
    "    for location in locations:\n",
    "        for time in times:\n",
    "            calendar_map_schema.loc[len(calendar_map_schema)] = [location, time, sid, n_star_cmp(sid)]\n",
    "            sid += 1\n",
    "            \n",
    "    hash_id = 21\n",
    "    for location in location_star_keys:\n",
    "        calendar_map_schema.loc[len(calendar_map_schema)] = [location, '*', sid, n_star_cmp(sid)]\n",
    "        sid += 1\n",
    "    \n",
    "    for time in time_star_keys:\n",
    "        calendar_map_schema.loc[len(calendar_map_schema)] = ['*', time, sid, n_star_cmp(sid)]\n",
    "        sid += 1\n",
    "\n",
    "    calendar_map_schema.loc[len(calendar_map_schema)] = ['*', '*', sid, n_star_cmp(sid)]\n",
    "\n",
    "    return calendar_map_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0\n",
    "def get_transactions_from_dataset():\n",
    "    transactions = pd.DataFrame(columns=['transaction','location','time'])\n",
    "    \n",
    "    test_data = [\n",
    "        [['I2','I3'], 'S1', 'T1'],\n",
    "        [['I1','I3'], 'S1', 'T2'],\n",
    "        [['I1','I3','I4'], 'S1', 'T2'],\n",
    "        [['I1', 'I2'], 'S2', 'T2'],\n",
    "        [['I2', 'I4'], 'S2', 'T2'],\n",
    "        [['I2'], 'S1', 'T1']\n",
    "    ]\n",
    "    for datax in test_data:\n",
    "        transactions.loc[len(transactions)] = datax\n",
    "    \n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "def get_freq_2_itemset(transactions, locations, times, calendar_map_schema): \n",
    "    freq_2_items = pd.DataFrame(columns=['2-itemset','count','location','time','hash'])\n",
    "    for location in locations:\n",
    "        for time in times:\n",
    "            rows = transactions.loc[(transactions['location'] == location) & (transactions['time'] == time)]\n",
    "            rows = rows.query('transaction.str.len() >= 2')\n",
    "            \n",
    "            if len(rows) == 0:\n",
    "                continue\n",
    "            \n",
    "            itemset_dict = defaultdict(lambda: 0)\n",
    "            \n",
    "            transaction_items = []\n",
    "            for index in rows.index.values:\n",
    "                row = rows.loc[index]\n",
    "                transaction_item = row['transaction']\n",
    "                transaction_items.extend(transaction_item)\n",
    "             \n",
    "            split_transactions = list(combinations(set(transaction_items), 2))\n",
    "            \n",
    "            for split_trans in split_transactions:\n",
    "                split_trans_1 = list(split_trans)\n",
    "                split_trans_1.sort()\n",
    "                for row_trans in rows['transaction'].values:\n",
    "                    if set(split_trans_1).issubset(set(row_trans)):\n",
    "                        itemset_dict[tuple(split_trans_1)] += 1\n",
    "                    else:\n",
    "                        itemset_dict[tuple(split_trans_1)] += 0\n",
    "                    \n",
    "            calendar_entry_hash = calendar_map_schema.loc[(calendar_map_schema['location'] == location) & (calendar_map_schema['time'] == time)]['hash_id']\n",
    "            hash_id = calendar_entry_hash[calendar_entry_hash.index.values[0]]\n",
    "            new_row = [[],[], location, time, hash_id]\n",
    "            for key,value in itemset_dict.items():\n",
    "                new_row[0].append(list(key))\n",
    "                new_row[1].append(value)\n",
    "                \n",
    "            freq_2_items.loc[len(freq_2_items)] = new_row\n",
    "    return freq_2_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "def get_1_star_cmp(freq_2_itemset, calendar_map_schema):\n",
    "    n1_star_cmp = pd.DataFrame(columns=['partition','2-itemset','supp','hash_1_star','count'])\n",
    "    \n",
    "    for index in freq_2_itemset.index.values:\n",
    "        row = freq_2_itemset.loc[index]\n",
    "        hash_1_star = []\n",
    "        location = row['location']\n",
    "        time = row['time']\n",
    "        hash_1_star_loc = calendar_map_schema.loc[(calendar_map_schema['location'] == location) & (calendar_map_schema['time'] == '*')]\n",
    "        hash_1_star_time = calendar_map_schema.loc[(calendar_map_schema['location'] == '*') & (calendar_map_schema['time'] == time)]\n",
    "        new_count = []\n",
    "        if not hash_1_star_loc.empty:\n",
    "            hash_id = hash_1_star_loc['hash_id'][hash_1_star_loc.index.values[0]]\n",
    "            hash_1_star.append(hash_id)\n",
    "            new_count.append(1)\n",
    "        \n",
    "        if not hash_1_star_time.empty:\n",
    "            hash_id = hash_1_star_time['hash_id'][hash_1_star_time.index.values[0]]\n",
    "            hash_1_star.append(hash_id)\n",
    "            new_count.append(1)\n",
    "        \n",
    "        n2_itemsets = row['2-itemset']\n",
    "        counts = row['count']\n",
    "        \n",
    "        for itemset, count in zip(n2_itemsets, counts):\n",
    "            if count == 0:\n",
    "                continue\n",
    "            new_row = [index, n2_itemsets, count, hash_1_star, new_count]\n",
    "            n1_star_cmp.loc[len(n1_star_cmp)] = new_row\n",
    "        \n",
    "    return n1_star_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "def get_2_star_cmp(n1_star_cmp, cms):\n",
    "    n2_star_cmp = pd.DataFrame(columns=['partition','2-itemset','supp','hash_2_star','count'])\n",
    "    for index in n1_star_cmp.index.values:\n",
    "        row = n1_star_cmp.loc[index]\n",
    "        hash_1_star = row['hash_1_star']\n",
    "        new_row = copy.deepcopy(row)\n",
    "        new_row = new_row.drop('hash_1_star')\n",
    "        \n",
    "        cms_2_star_hash = cms.loc[(cms['location'] == '*') & (cms['time'] == '*')]['hash_id']\n",
    "        hash_id = cms_2_star_hash[cms_2_star_hash.index.values[0]]\n",
    "        \n",
    "        new_row['hash_2_star'] = [hash_id for i in range(len(hash_1_star))]\n",
    "        n2_star_cmp.loc[len(n2_star_cmp)] = new_row\n",
    "    \n",
    "    return n2_star_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "def get_freq_3_itemset(freq_2_itemset, n1_star_cmp):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6\n",
    "def get_freq_3_itemset_1_star(freq_3_itemset, n1_star_cmp):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7\n",
    "def get_freq_3_itemset_2_star(freq_3_itemset_1_star, n2_star_cmp):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    locations = np.array(['S1','S2','S3'])\n",
    "    times = np.array(['T1', 'T2'])\n",
    "    items = np.array(['I1', 'I2', 'I3', 'I4', 'I5'])\n",
    "    \n",
    "    transactions = get_transactions_from_dataset()\n",
    "#     pollutants = []\n",
    "#     dict_generator(locations, pollutants, times)\n",
    "    calendar_map_schema = calendar_map_schema_generator(transactions)\n",
    "    \n",
    "    #replace calendarmapschema with just hashes\n",
    "    freq_2_itemset = get_freq_2_itemset(transactions, locations, times, calendar_map_schema)\n",
    "    \n",
    "    n1_star_cmp = get_1_star_cmp(freq_2_itemset, calendar_map_schema)\n",
    "    \n",
    "    n2_star_cmp = get_2_star_cmp(n1_star_cmp, calendar_map_schema)\n",
    "    \n",
    "    freq_3_itemset = get_freq_3_itemset(freq_2_itemset, n1_star_cmp)\n",
    "    \n",
    "    freq_3_itemset_1_star = get_freq_3_itemset_1_star(freq_3_itemset, n1_star_cmp)\n",
    "    \n",
    "    freq_3_itemset_2_star = get_freq_3_itemset_2_star(freq_3_itemset_1_star, n2_star_cmp)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
