{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_star_cmp(n):\n",
    "#     return (n+1)*unique_id\n",
    "    return (n+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_is_subset(parent_list, child_list):\n",
    "    return set(child_list).issubset(set(parent_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summary: 3-D matrix (locations, pollutants, quartiles)\n",
    "## samples: 2-D matrix (locations, pollutants)\n",
    "## pollutant_names: 1-D array\n",
    "def preprocessing(samples, summary, pollutant_names):\n",
    "    database_codes = []\n",
    "\n",
    "    for i in len(dataset):\n",
    "        segment = dataset[i]\n",
    "        for j in len(segment):\n",
    "            attribute_value = segment[j]\n",
    "\n",
    "            minimum = summary[i][j][0]\n",
    "            first_quantile = summary[i][j][1]\n",
    "            third_quantile = summary[i][j][3]\n",
    "\n",
    "            if (attribute_value >= minimum) and (attribute_value < first_quantile):\n",
    "                database_codes[i][j] = pollutant_names[j] + \"1\"\n",
    "            elif (attribute_value >= first_quantile) and (attribute_value < third_quantile):\n",
    "                database_codes[i][j] = pollutant_names[j] + \"2\"\n",
    "            else:\n",
    "                database_codes[i][j] = pollutant_names[j] + \"3\"\n",
    "\n",
    "    return database_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## locations: 1-D array containing names of locations\n",
    "## pollutants: 1-D array containing names of pollutants\n",
    "## times: 1-D array containing names of time measures (January, February etc.)\n",
    "def dict_generator(locations, pollutants, times):\n",
    "    for i in range(len(locations)):\n",
    "        location_dict[locations[i]] = i\n",
    "    for j in range(len(times)):\n",
    "        time_dict[times[j]] = j\n",
    "    for k in range(len(pollutants)):\n",
    "        pollutant_dict[pollutants[k]] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite to decrease complexity\n",
    "def get_hash_id_from_cms(cms, location, time, no_of_stars):\n",
    "    hash_id_entry = cms.loc[(cms['location'] == location) & (cms['time'] == time)]\n",
    "    hash_id = None\n",
    "    if not (hash_id_entry.empty) and (no_of_stars >= 0):\n",
    "        hash_id = hash_id_entry['hash_id'][hash_id_entry.index.values[0]]\n",
    "    else:\n",
    "        hash_id_entry_loc_and_star = cms.loc[(cms['location'] == location) & (cms['time'] == '*')]\n",
    "        hash_id_entry_time_and_star = cms.loc[(cms['location'] == '*') & (cms['time'] == time)]\n",
    "        \n",
    "        if not (hash_id_entry_loc_and_star.empty) and (no_of_stars >= 1):\n",
    "            hash_id = hash_id_entry_loc_and_star['hash_id'][hash_id_entry_loc_and_star.index.values[0]]\n",
    "        elif not (hash_id_entry_time_and_star.empty) and (no_of_stars >= 1):\n",
    "            hash_id = hash_id_entry_time_and_star['hash_id'][hash_id_entry_time_and_star.index.values[0]]\n",
    "        else:\n",
    "            hash_id_entry_star_and_star = cms.loc[(cms['location'] == '*') & (cms['time'] == '*')]\n",
    "            if not (hash_id_entry_star_and_star.empty) and (no_of_stars >= 2):\n",
    "                hash_id = hash_id_entry_star_and_star['hash_id'][hash_id_entry_star_and_star.index.values[0]]\n",
    "    \n",
    "    return hash_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0\n",
    "def get_transactions_from_dataset():\n",
    "    transactions = pd.DataFrame(columns=['transaction','location','time'])\n",
    "    \n",
    "    test_data = [\n",
    "        [['I2','I3'], 'S1', 'T1'],\n",
    "        [['I1','I3'], 'S1', 'T2'],\n",
    "        [['I1','I3','I4'], 'S1', 'T2'],\n",
    "        [['I1', 'I2'], 'S2', 'T2'],\n",
    "        [['I2', 'I4'], 'S2', 'T2'],\n",
    "        [['I2'], 'S1', 'T1']\n",
    "    ]\n",
    "    for datax in test_data:\n",
    "        transactions.loc[len(transactions)] = datax\n",
    "    \n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "## locations: 1-D array containing names of locations\n",
    "## times: 1-D array containing names of time measures (January, February etc.)\n",
    "def calendar_map_schema_generator(transactions):\n",
    "    \n",
    "    # correct it to add only stars for possible locations and times. e.g. s2 doesnt need stars\n",
    "    locations = transactions['location'].unique()\n",
    "    times = transactions['time'].unique()\n",
    "    \n",
    "    location_star_keys = set()\n",
    "    for location in locations:\n",
    "        rows = transactions.loc[transactions['location'] == location]\n",
    "        unique_times = rows['time'].unique()\n",
    "        if len(unique_times) > 1:\n",
    "            location_star_keys.add(location)\n",
    "    \n",
    "    time_star_keys = set()\n",
    "    for time in times:\n",
    "        rows = transactions.loc[transactions['time'] == time]\n",
    "        unique_locations = rows['location'].unique()\n",
    "        if len(unique_locations) > 1:\n",
    "            time_star_keys.add(time)\n",
    "    \n",
    "    location_stars = np.array(['*' for i in range(len(location_star_keys))])\n",
    "    time_stars = np.array(['*' for i in range(len(time_star_keys))])\n",
    "    \n",
    "    calendar_map_schema = pd.DataFrame(columns=['location', 'time', 'sid', 'hash_id'])\n",
    "    \n",
    "    sid = 0\n",
    "    for location in locations:\n",
    "        for time in times:\n",
    "            calendar_map_schema.loc[len(calendar_map_schema)] = [location, time, sid, n_star_cmp(sid)]\n",
    "            sid += 1\n",
    "            \n",
    "    hash_id = 21\n",
    "    for location in location_star_keys:\n",
    "        calendar_map_schema.loc[len(calendar_map_schema)] = [location, '*', sid, n_star_cmp(sid)]\n",
    "        sid += 1\n",
    "    \n",
    "    for time in time_star_keys:\n",
    "        calendar_map_schema.loc[len(calendar_map_schema)] = ['*', time, sid, n_star_cmp(sid)]\n",
    "        sid += 1\n",
    "\n",
    "    calendar_map_schema.loc[len(calendar_map_schema)] = ['*', '*', sid, n_star_cmp(sid)]\n",
    "\n",
    "    return calendar_map_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "def get_freq_2_itemset(transactions, locations, times, calendar_map_schema): \n",
    "    freq_2_items = pd.DataFrame(columns=['2-itemset','count','location','time','hash_id'])\n",
    "    for location in locations:\n",
    "        for time in times:\n",
    "            rows = transactions.loc[(transactions['location'] == location) & (transactions['time'] == time)]\n",
    "            rows = rows.query('transaction.str.len() >= 2')\n",
    "            \n",
    "            if len(rows) == 0:\n",
    "                continue\n",
    "            \n",
    "            itemset_dict = defaultdict(lambda: 0)\n",
    "            \n",
    "            transaction_items = []\n",
    "            for index in rows.index.values:\n",
    "                row = rows.loc[index]\n",
    "                transaction_item = row['transaction']\n",
    "                transaction_items.extend(transaction_item)\n",
    "             \n",
    "            split_transactions = list(combinations(set(transaction_items), 2))\n",
    "            \n",
    "            for split_trans in split_transactions:\n",
    "                split_trans_1 = list(split_trans)\n",
    "                split_trans_1.sort()\n",
    "                for row_trans in rows['transaction'].values:\n",
    "                    if set(split_trans_1).issubset(set(row_trans)):\n",
    "                        itemset_dict[tuple(split_trans_1)] += 1\n",
    "                    else:\n",
    "                        itemset_dict[tuple(split_trans_1)] += 0\n",
    "                    \n",
    "            calendar_entry_hash = calendar_map_schema.loc[(calendar_map_schema['location'] == location) & (calendar_map_schema['time'] == time)]['hash_id']\n",
    "            hash_id = calendar_entry_hash[calendar_entry_hash.index.values[0]]\n",
    "            new_row = [[],[], location, time, hash_id]\n",
    "            for key,value in itemset_dict.items():\n",
    "                new_row[0].append(list(key))\n",
    "                new_row[1].append(value)\n",
    "                \n",
    "            freq_2_items.loc[len(freq_2_items)] = new_row\n",
    "    return freq_2_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "def get_1_star_cmp(freq_2_itemset, calendar_map_schema):\n",
    "    n1_star_cmp = pd.DataFrame(columns=['partition','2-itemset','supp','hash_1_star','count'])\n",
    "    \n",
    "    for index in freq_2_itemset.index.values:\n",
    "        row = freq_2_itemset.loc[index]\n",
    "        hash_1_star = []\n",
    "        location = row['location']\n",
    "        time = row['time']\n",
    "        hash_1_star_loc = calendar_map_schema.loc[(calendar_map_schema['location'] == location) & (calendar_map_schema['time'] == '*')]\n",
    "        hash_1_star_time = calendar_map_schema.loc[(calendar_map_schema['location'] == '*') & (calendar_map_schema['time'] == time)]\n",
    "        new_count = []\n",
    "        if not hash_1_star_loc.empty:\n",
    "            hash_id = hash_1_star_loc['hash_id'][hash_1_star_loc.index.values[0]]\n",
    "            hash_1_star.append(hash_id)\n",
    "            new_count.append(1)\n",
    "        \n",
    "        if not hash_1_star_time.empty:\n",
    "            hash_id = hash_1_star_time['hash_id'][hash_1_star_time.index.values[0]]\n",
    "            hash_1_star.append(hash_id)\n",
    "            new_count.append(1)\n",
    "        \n",
    "        n2_itemsets = row['2-itemset']\n",
    "        supps = row['count']\n",
    "        \n",
    "        for itemset, supp in zip(n2_itemsets, supps):\n",
    "            hash_1_star_to_insert = [] if supp == 0 else hash_1_star\n",
    "            count_to_insert = [0] if supp == 0 else new_count\n",
    "            new_row = [index, itemset, supp, hash_1_star_to_insert, count_to_insert]\n",
    "            n1_star_cmp.loc[len(n1_star_cmp)] = new_row\n",
    "        \n",
    "    return n1_star_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "def get_2_star_cmp(n1_star_cmp, cms):\n",
    "    n2_star_cmp = pd.DataFrame(columns=['partition','2-itemset','supp','hash_2_star','count'])\n",
    "    for index in n1_star_cmp.index.values:\n",
    "        row = n1_star_cmp.loc[index]\n",
    "        hash_1_star = row['hash_1_star']\n",
    "        new_row = copy.deepcopy(row)\n",
    "        new_row = new_row.drop('hash_1_star')\n",
    "        \n",
    "        cms_2_star_hash = cms.loc[(cms['location'] == '*') & (cms['time'] == '*')]['hash_id']\n",
    "        hash_id = cms_2_star_hash[cms_2_star_hash.index.values[0]]\n",
    "        \n",
    "        new_row['hash_2_star'] = [hash_id for i in range(len(hash_1_star))]\n",
    "        n2_star_cmp.loc[len(n2_star_cmp)] = new_row\n",
    "    \n",
    "    return n2_star_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "def get_freq_3_itemset(freq_2_itemset):\n",
    "    freq_3_itemset = pd.DataFrame(columns=['partition', '3-itemset', 'location', 'time', 'hash_id'])\n",
    "    \n",
    "    for index in freq_2_itemset.index.values:\n",
    "        row = freq_2_itemset.loc[index]\n",
    "        \n",
    "        location = row['location']\n",
    "        time = row['time']            \n",
    "        old_itemsets = row['2-itemset']\n",
    "        flatten = lambda l: [item for items in l for item in items]\n",
    "        new_itemsets = set(flatten(old_itemsets))\n",
    "        new_itemsets_comb = list(combinations(new_itemsets, 3))\n",
    "        for itemset in new_itemsets_comb:\n",
    "            new_itemset = list(itemset)\n",
    "            new_itemset.sort()\n",
    "            new_row = [index, new_itemset, row['location'], row['time'], row['hash_id']]\n",
    "            freq_3_itemset.loc[len(freq_3_itemset)] = new_row\n",
    "            \n",
    "            \n",
    "    return freq_3_itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6\n",
    "def get_freq_3_itemset_1_star(freq_3_itemset, n1_star_cmp):\n",
    "    freq_3_itemset_1_star = pd.DataFrame(columns=['partition', '3-itemset', 'supp', 'hash_1_star', 'count'])\n",
    "    \n",
    "    for index in freq_3_itemset.index.values:\n",
    "        row = freq_3_itemset.loc[index]\n",
    "        \n",
    "        n1_star_cmp_rows = n1_star_cmp.loc[n1_star_cmp['partition'] == row['partition']]\n",
    "        \n",
    "        min_count = 1000\n",
    "        star_row_chosen = None\n",
    "        for star_index, star_row in n1_star_cmp_rows.iterrows():\n",
    "            count = min(star_row['count'])\n",
    "            if (count < min_count):\n",
    "                star_row_chosen = star_row\n",
    "                min_count = count\n",
    "        \n",
    "        new_row = [row['partition'], row['3-itemset'], star_row_chosen['supp'], star_row_chosen['hash_1_star'], star_row_chosen['count']]\n",
    "        freq_3_itemset_1_star.loc[len(freq_3_itemset_1_star)] = new_row\n",
    "        \n",
    "    return freq_3_itemset_1_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7\n",
    "def get_freq_3_itemset_2_star(freq_3_itemset_1_star, n2_star_cmp):\n",
    "    freq_3_itemset_2_star = pd.DataFrame(columns=['partition', '3-itemset', 'supp', 'hash_2_star', 'count'])\n",
    "    \n",
    "    for index, row in freq_3_itemset_1_star.iterrows():\n",
    "        n2_star_cmp_rows = n2_star_cmp.loc[n2_star_cmp['partition'] == row['partition']]\n",
    "        \n",
    "        min_count = 1000\n",
    "        star_row_chosen = None\n",
    "        for star_index, star_row in n2_star_cmp_rows.iterrows():\n",
    "            count = min(star_row['count'])\n",
    "            if (count < min_count):\n",
    "                star_row_chosen = star_row\n",
    "                min_count = count\n",
    "        \n",
    "        new_row = copy.deepcopy(row)\n",
    "        new_row.drop('hash_1_star')\n",
    "        new_row['hash_2_star'] = star_row_chosen['hash_2_star']\n",
    "        new_row['supp'] = star_row_chosen['supp']\n",
    "        \n",
    "        freq_3_itemset_2_star.loc[len(freq_3_itemset_2_star)] = new_row\n",
    "    \n",
    "    return freq_3_itemset_2_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    locations = np.array(['S1','S2','S3'])\n",
    "    times = np.array(['T1', 'T2'])\n",
    "    items = np.array(['I1', 'I2', 'I3', 'I4', 'I5'])\n",
    "    \n",
    "    transactions = get_transactions_from_dataset()\n",
    "    calendar_map_schema = calendar_map_schema_generator(transactions)\n",
    "    \n",
    "    #replace calendarmapschema with just hashes\n",
    "    freq_2_itemset = get_freq_2_itemset(transactions, locations, times, calendar_map_schema)\n",
    "    \n",
    "    n1_star_cmp = get_1_star_cmp(freq_2_itemset, calendar_map_schema)\n",
    "    \n",
    "    n2_star_cmp = get_2_star_cmp(n1_star_cmp, calendar_map_schema)\n",
    "    \n",
    "    freq_3_itemset = get_freq_3_itemset(freq_2_itemset)\n",
    "    \n",
    "    freq_3_itemset_1_star = get_freq_3_itemset_1_star(freq_3_itemset, n1_star_cmp)\n",
    "    \n",
    "    freq_3_itemset_2_star = get_freq_3_itemset_2_star(freq_3_itemset_1_star, n2_star_cmp)\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
